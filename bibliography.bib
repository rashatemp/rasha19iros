@STRING{aaai    = {Proc.\ of the Conf.~on Advancements of Artificial Intelligence (AAAI)} }
@STRING{aaaiold = {Proc.\ of the National Conf.~on Artificial Intelligence (AAAI)} }
@STRING{ac      = {IEEE Trans. on Automatic Control} }
@STRING{acc     = {Proc.\ of the IEEE American Control onf(ACC)} }
@STRING{accv    = {Proc.\ of the Asian Conf.\ on Computer Vision (ACCV)} }
@STRING{acra    = {Proc.\ of the Australasian Conf.\ on Robotics and Automation (ACRA)} }
@STRING{acmgraphics = {ACM Transactions on Graphics} }
@STRING{addison = {Addison-Wesley Publishing Inc.} }
@STRING{advancedrobotics={Advanced Robotics} }
@STRING{ai      = {Artificial Intelligence} }
@STRING{ams     = {Proc.\ of Autonome Mobile Systeme} }
@STRING{ar      = {Autonomous Robots} }
@STRING{arxiv   = {arXiv preprint} }
@STRING{bmvc    = {Proc.\ of British Machine Vision onf(BMVC)} }
@STRING{cacm    = {Communications of the ACM} }
@STRING{ccvw    = {Proc.\ of the Croation Computer Vision Workshop (CCVW)} }
@STRING{cira    = {Proc.\ of the IEEE Intl.\ Symp. on Computer Intelligence in Robotics and Automation (CIRA)} }
@STRING{cogsys  = {Proc.\ of the Intl.\ Conf.\ on Cognitive Systems (CogSys)} }
@STRING{cviu    = {Journal of Computer Vision and Image Understanding (CVIU)} }
@STRING{cvpr    = {Proc.\ of the IEEE Conf.\ on Computer Vision and Pattern Recognition (CVPR)} }
@STRING{cvvt    = {Proc.\ of the Intl.\ Workshop on Computer Vision in Vehicle Technology (CVVT)} }
@STRING{dagm    = {Proc.\ of the Symposion of the German Association for Pattern Recognition (DAGM)} }
@STRING{dagstuhl= {Proc.\ of the Dagstuhl Seminar} }
@STRING{dgpf    = {Proc.\ of the Conf.\ of the German Society for Photogrammetry, Remote Sensing and Geoinformation (DGPF)} }
@STRING{eccv    = {Proc.\ of the Europ.\ Conf.\ on Computer Vision (ECCV)} }
@STRING{ecmr    = {Proc.\ of the Europ.\ Conf.\ on Mobile Robotics (ECMR)} }
@STRING{emav    = {Proc.\ of the European Micro Aerial Vehicle Conference} }
@STRING{euros   = {Proc.\ of the Europ.\ Robotics Symp. (EUROS)} }
@STRING{fntr    = {Foundations and Trends in Robotics} }
@STRING{gcpr    = {Proc.\ of the German Conf.\ on Pattern Recognition (GCPR)} }
@STRING{humanoids={Proc.\ of the IEEE Intl.\ Conf.\ on Humanoid Robots} }
@STRING{icann   = {Proc.\ of the Intl.\ Conf.\ on Artificial Neural Networks (ICANN)}}
@STRING{icar    = {Proc.\ of the Intl.\ Conf.\ on Advanced Robotics (ICAR)} }
@STRING{iccas   = {Proc.\ of the Int.\ Conf.\ on Control, Automation and Systems (ICCAS)}}
@STRING{iccv    = {Proc.\ of the IEEE Intl.\ Conf.\ on Computer Vision (ICCV)} }
@STRING{iciap   = {Proc.\ of the Intl.\ Conf.\ on Image Analysis and Processing (ICIAP)} }
@STRING{icias   = {Proc.\ of the Int.\ Conf.\ on Intelligent Autonomous Systems (ICIAS)}}
@STRING{icip    = {Proc.\ of the IEEE Intl.\ Conf.\ on Image Processing (ICIP)} }
@STRING{icra    = {Proc.\ of the IEEE Intl.\ Conf.\ on Robotics \& Automation (ICRA)} }
@STRING{icuas   = {Proc.\ of the Intl.\ Conf.\ on Unmanned Aircraft Systems (ICUAS)} }
@STRING{ieeepress={IEEE Computer Society Press} }
@STRING{ijcai   = {Proc.\ of the Intl.\ Conf.\ on Artificial Intelligence (IJCAI)} }
@STRING{ijcv    = {Intl.\ Journal of Computer Vision (IJCV)} }
@STRING{ijgi    = {Intl.\ Journal of Geo-Information} }
@STRING{ijhr    = {The Int. Journal of Humanoid Robotics (IJHR)} }
@STRING{ijrr    = {Intl.\ Journal of Robotics Research (IJRR)} }
@STRING{imvip   = {Proc.\ of the Irish Machine Vision and Image Processing onf(IMVIP)} }
@STRING{iros    = {Proc.\ of the IEEE/RSJ Intl.\ Conf.\ on Intelligent Robots and Systems (IROS)} }
@STRING{iser    = {Proc.\ of the Intl.\ Symp.\ on Experimental Robotics (ISER)} }
@STRING{ismar   = {Proc.\ of the Intl.\ Symp.\ on Mixed and Augmented Reality (ISMAR)} }
@STRING{isprsannals={ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences} }
@STRING{isprsarchives={ISPRS Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences} }
@STRING{isrr    = {Proc.\ of the Intl.\ Symp.\ on Robotic Research (ISRR)} }
@STRING{iv      = {Proc.\ of the IEEE Intelligent Vehicles Symposium (IV)} }
@STRING{ivc     = {Journal on Image and Vision Computing (IVC)} }
@STRING{jair    = {Journal of Artificial Intelligence Research (JAIR)} }
@STRING{jbe     = {ASME Journal of Basic Engineering} }
@STRING{jfr     = {Journal of Field Robotics (JFR)} }
@STRING{jirs    = {Journal of Intelligent and Robotic Systems (JIRS)} }
@STRING{jmiv    = {Journal of Mathematical Imaging and Vision} }
@STRING{joe     = {IEEE Journal of Oceanic Engineering} }
@STRING{jprs    = {ISPRS Journal of Photogrammetry and Remote Sensing (JPRS)} }
@STRING{jra     = {IEEE Journal of Robotics and Automation} }
@STRING{jras    = {Journal on Robotics and Autonomous Systems (RAS)} }
@STRING{mcg     = {Proc.\ of the Intl.\ Conf.\ on Machine Control and Guidance (MCG)} }
@STRING{mfi     = {Proc.\ of the {IEEE} Int.\ Conf.\ on Multisensor Fusion and Integration for Intelligent Systems (MFI)}}
@STRING{mirage  = {Proc.\ of the Intl.\ Conf.\ on Computer Vision/Computer Graphics Collaboration Techniques and Applications (MIRAGE)} }
@STRING{mitpress= {MIT Press} }
@STRING{ml      = {Machine Learning} }
@STRING{mobicom = {Proc.\ of the {ACM} Intl.\ Conf.\ on Mobile Computing and Networking (MobiCom)} }
@STRING{mva     = {Proc.\ of the IAPR Conf.\ on Machine Vision Applications (MVA)} }
@STRING{nips    = {Proc.\ of the Conf.\ on Neural Information Processing Systems (NIPS)} }
@STRING{nipsjournal={Advances in Neural Information Processing Systems} }
@STRING{oceans  = {Proc.\ of OCEANS MTS/IEEE onfand Exhibition} }
@STRING{pami    = {IEEE Trans.\ on Pattern Analalysis and Machine Intelligence (TPAMI)} }
@STRING{pcv     = {Proc.\ of the ISPRS Conf.~on Photogrammeric Computer Vision (PCV)} }
@STRING{pers    = {Photogrammetric Engineering and Remote Sensing (PE\&RS)} }
@STRING{pfg     = {Photogrammetrie -- Fernerkundung -- Geoinformation (PFG)} }
@STRING{phowo   = {Proc.\ of the Photogrammetric Week (PhoWo)} }
@STRING{pia     = {Proc.\ of the ISPRS Conf.~on Photogrammeric Image Analysis (PIA)} }
@STRING{pr      = {Pattern Recognition} }
@STRING{prl     = {Pattern Recognition Letters} }
@STRING{ral     = {IEEE Robotics and Automation Letters (RA-L)} }
@STRING{ram     = {IEEE Robotics and Automation Magazine (RAM)} }
@STRING{ras     = {Journal on Robotics and Autonomous Systems (RAS)} }
@STRING{rasmag  = {IEEE Robotics and Automation Magazine} }
@STRING{rs      = {Remote Sensing} }
@STRING{rss     = {Proc.\ of Robotics: Science and Systems (RSS)} }
@STRING{rssbook = {Robotics: Science and Systems} }
@STRING{sensors = {IEEE Sensors Journal} }
@STRING{sice    = {Proc.\ of the Annual onfof the Society of Instrument and Control Engineers (SICE)} }
@STRING{smc     = {Proc.\ of the IEEE Intl.\ Conf.\ on Systems, Man, and Cybernetics (SMC)} }
@STRING{snowbird= {Proc.\ of the Learning Workshop (Snowbird)} }
@STRING{soave   = {Proc.\ of the Workshop on Self-Organization of AdaptiVE behavior (SOAVE)} }
@STRING{spiesdvrs={Proc.\ of SPIE Stereoscopic Displays and Virtual Reality Systems} }
@STRING{spiev   = {Proc.\ of SPIE Videometrics} }
@STRING{springer= {Springer Verlag} }
@STRING{springerstaradvanced={STAR Springer Tracts in Advanced Robotics} }
@STRING{tarj    = {The Australian Rangeland Journal} }
@STRING{taros   = {Proc.\ of the Conf.\ Towards Autonomous Robotic Systems (TAROS)}}
@STRING{tits    = {IEEE Trans.\ on Intelligent Transportation Systems (ITS)} }
@STRING{titsmag = {IEEE Trans.\ on Intelligent Transportation Systems Magazine} }
@STRING{tpami   = {IEEE Trans.\ on Pattern Analalysis and Machine Intelligence (TPAMI)} }
@STRING{tra     = {IEEE Trans.\ on Robotics and Automation} }
@STRING{tro     = {IEEE Trans.\ on Robotics (TRO)} }
@STRING{uai     = {Proc.\ of the Conf.\ on Uncertainty in Artificial Intelligence (UAI)} }
@STRING{uavg    = {Proc.\ of the Intl.\ Conf.\ on Unmanned Aerial Vehicles in Geomatics} }
@STRING{uust    = {Proc.\ of the Intl.\ Symp.\ on Unmanned Untethered Submersible Technology} }
@STRING{vc      = {The Visual Computer (VC)} }
@STRING{wafr    = {Intl.\ Workshop on the Algorithmic Foundations of Robotics (WAFR)} }


@techreport{settles2009active,
  title={Active learning literature survey},
  author={Settles, Burr},
  year={2009},
  institution={Univ.~of Wisconsin-Madison, Dep.~of Computer Sciences}
}

@inproceedings{guyon2011results,
  title={Results of the active learning challenge},
  author={Guyon, Isabelle and Cawley, Gavin and Dror, Gideon and Lemaire, Vincent},
  booktitle={Proc.~of the AISTATS Active Learning and Experimental Design Workshop},
  pages={19--45},
  year={2011}
}

@inproceedings{holub2008entropy,
  title={Entropy-based active learning for object recognition},
  author={Holub, Alex and Perona, Pietro and Burl, Michael C.},
  booktitle={IEEE Computer Society Conf.~on Computer Vision and Pattern Recognition Workshops},
  pages={1--8},
  year={2008},
}

@inproceedings{zhou2017fine,
  title={Fine-tuning convolutional neural networks for biomedical image analysis: actively and incrementally},
  author={Zhou, Zongwei and Shin, Jae and Zhang, Lei and Gurudu, Suryakanth and Gotway, Michael and Liang, Jianming},
  booktitle=cvpr,
  pages={7340--7351},
  year={2017}
}

@inproceedings{yang2017suggestive,
  title={Suggestive annotation: A deep active learning framework for biomedical image segmentation},
  author={Yang, Lin and Zhang, Yizhe and Chen, Jianxu and Zhang, Siyuan and Chen, Danny Z.},
  booktitle={Proc.~of the Intl.~Conf.~on Medical Image Computing and Computer-Assisted Intervention},
  pages={399--407},
  year={2017},
}

@inproceedings{dutt2016active,
  title={Active image segmentation propagation},
  author={Dutt Jain, Suyog and Grauman, Kristen},
  booktitle={Proc.~of the IEEE Conf.~on Computer Vision and Pattern Recognition},
  pages={2864--2873},
  year={2016}
}


@inproceedings{gal2017deep,
  title={Deep bayesian active learning with image data},
  author={Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
  booktitle={Proc.~of thhe Intl.~Conf.~on Machine Learning},
  pages={1183--1192},
  year={2017},
}


@article{milioto2018bonnet,
  title={Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics using CNNs},
  author={Milioto, Andres and Stachniss, Cyrill},
  journal={Proc.~of the IEEE Intl.~Conf.~on Robotics and Automation (ICRA)},
  year={2019}
}


@inproceedings{milioto2018real,
  title={Real-time semantic segmentation of crop and weed for precision agriculture robots leveraging background knowledge in cnns},
  author={Milioto, Andres and Lottes, Philipp and Stachniss, Cyrill},
  booktitle={Proc.~of the IEEE Intl.~Conf.~on Robotics and Automation (ICRA)},
  pages={2229--2235},
  year={2018},
}

@inproceedings{beluch2018power,
  title={The power of ensembles for active learning in image classification},
  author={Beluch, William H. and Genewein, Tim and N{\"u}rnberger, Andreas and K{\"o}hler, Jan M.},
  booktitle={Proc.~of the IEEE Conf.~on Computer Vision and Pattern Recognition},
  pages={9368--9377},
  year={2018}
}

@article{sener2017geometric,
  title={A geometric approach to active learning for convolutional neural networks},
  author={Sener, Ozan and Savarese, Silvio},
  journal={arXiv preprint arXiv},
  volume={1708},
  pages={1},
  year={2017}
}

@article{wang2017cost,
  title={Cost-effective active learning for deep image classification},
  author={Wang, Keze and Zhang, Dongyu and Li, Ya and Zhang, Ruimao and Lin, Liang},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={27},
  number={12},
  pages={2591--2600},
  year={2017},
}


@article{kading2016active,
  title={Active and continuous exploration with deep neural networks and expected model output changes},
  author={K{\"a}ding, Christoph and Rodner, Erik and Freytag, Alexander and Denzler, Joachim},
  journal={arXiv preprint arXiv:1612.06129},
  year={2016}
}


@inproceedings{freytag2014selecting,
  title={Selecting influential examples: Active learning with expected model output changes},
  author={Freytag, Alexander and Rodner, Erik and Denzler, Joachim},
  booktitle={European Conf.~on Computer Vision},
  pages={562--577},
  year={2014},
}

@InProceedings{lottes2017iros,
title = {Semi-Supervised Online Visual Crop and Weed Classification in Precision Farming Exploiting Plant Arrangement},
author = {Philipp Lottes and Cyrill Stachniss},
booktitle = iros,
year = {2017},
url = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/lottes17iros.pdf},
}

@Article{lottes2018ral,
  author        = {Philipp Lottes and Jens Behley and Andres Milioto and Cyrill Stachniss},
  title         = {Fully Convolutional Networks with Sequential Information for Robust Crop and Weed Detection in Precision Farming},
  journal       = ral,
  year          = 2018,
  volume        = 3,
  issue         = 4,
  pages         = {3097-3104},
  doi           = {10.1109/LRA.2018.2846289},
  url           = {https://arxiv.org/abs/1806.03412},
  abstract      = {Reducing the use of agrochemicals is an important component towards sustainable agriculture. Robots that can perform targeted weed control offer the potential to contribute to this goal, for example, through specialized weeding actions such as selective spraying or mechanical weed removal. A prerequisite of such systems is a reliable and robust plant classification system that is able to distinguish crop and weed in the field. A major challenge in this context is the fact that different fields show a large variability. Thus, classification systems have to robustly cope with substantial environmental changes with respect to weed pressure and weed types, growth stages of the crop, visual appearance, and soil conditions. In this paper, we propose a novel crop-weed classification system that relies on a fully convolutional network with an encoder-decoder structure and incorporates spatial information by considering image sequences. Exploiting the crop arrangement information that is observable from the image sequences enables our system to robustly estimate a pixel-wise labeling of the images into crop and weed, i.e., a semantic segmentation. We provide a thorough experimental evaluation, which shows that our system generalizes well to previously unseen fields under varying environmental conditions --- a key capability to actually use such systems in precision framing. We provide comparisons to other state-of-the-art approaches and show that our system substantially improves the accuracy of crop-weed classification without requiring a retraining of the model.},
}


@Article{lottes2017jfr,
title = {Effective Vision-based Classification for Separating Sugar Beets and Weeds for Precision Farming},
author = {Lottes, Philipp and H\"oferlin, Markus and Sander, Slawomir and Stachniss, Cyrill},
journal = {Journal of Field Robotics},
year = {2017},
volume = {34},
issue = {6},
pages = {1160-1178},
doi = {10.1002/rob.21675},
issn = {1556-4967},
timestamp = {2016.10.5},
url = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/lottes16jfr.pdf},
}

@article{chebrolu2017agricultural,
  title={Agricultural robot dataset for plant classification, localization and mapping on sugar beet fields},
  author={Chebrolu, Nived and Lottes, Philipp and Schaefer, Alexander and Winterhalter, Wera and Burgard, Wolfram and Stachniss, Cyrill},
  journal={The Intl.~Journal of Robotics Research},
  volume={36},
  number={10},
  pages={1045--1052},
  year={2017},
}


@article{badrinarayanan2017segnet,
  title={Segnet: A deep convolutional encoder-decoder architecture for image segmentation},
  author={Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  journal=tpami,
  volume={39},
  number={12},
  pages={2481--2495},
  year={2017},
}

@article{paszke2016enet,
  title={Enet: A deep neural network architecture for real-time semantic segmentation},
  author={Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
  journal={arXiv preprint arXiv:1606.02147},
  year={2016}
}

@inproceedings{zhang2018self,
  title={Self-learning to detect and segment cysts in lung CT images without manual annotation},
  author={Zhang, Ling and Gopalakrishnan, Vissagan and Lu, Le and Summers, Ronald M and Moss, Joel and Yao, Jianhua},
  booktitle={IEEE Intl.~Symposium on Biomedical Imaging (ISBI 2018)},
  pages={1100--1103},
  year={2018},
}

@Article{mccool2018ral,
  author        = {McCool, Chris and Beattie, James and Firn, Jennifer and Lehnert, Chris and Kulk, Jason and Russell, Raymond and Perez, Tristan and Bawden, Owen},
  title         = {Efficacy of Mechanical Weeding Tools: A Study into Alternative Weed Management Strategies Enabled by Robotics},
  journal       = ral,
  year          = 2018,
  keywords      = {Agricultural Automation, Robotics in Agriculture and Forestry, Field Robots},
  abstract      = {The rise of herbicide resistant weed species has re-invigorated research in non-chemical methods for weed management. Robots, such as AgBot II, which can detect and classify weeds as they traverse a field are a key enabling factor for individualised treatment of weed species. Integral to the invidualised treatment of weed species are the the non-herbicide methods through which the weeds are managed. This paper explores mechanical methods as an alternative to weed management. Three implements are considered: belowsurface tilling (arrow hoe), above-surface tilling (tines) and a cutting mechanism. These mechanisms were evaluated in a controlled field with varying rates of application to herbicide resistant weeds of interest for Queensland, Australia. Statistical analysis demonstrated the efficacy of these implements and highlighted the importance of early intervention. It was found that a tine, deployed automatically on AgBot II, was effective for all of the weeds considered in this study, leading to an overall survival probability of 0.28 0.15. Further analysis demonstrated the significance of treatment time with late intervention commencing at week 6 resulting in a survival probability of 0.54 0.08 vs 0.24 0.18 for earlier intervention at week 4.},
}

@article{ducket2018arxiv,
  author    = {Duckett, Tom and
               Pearson, Simon and
               Blackmore, Simon and
               Grieve, Bruce},
  title     = {Agricultural Robotics: The Future of Robotic Agriculture},
  journal   = {arXiv preprint},
  volume    = {abs/1806.06762},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.06762},
  archivePrefix = {arXiv},
  eprint    = {1806.06762},
  timestamp = {Mon, 13 Aug 2018 16:46:25 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1806-06762},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{liebisch2016wslw,
  author        = {Liebisch, Frank and Pfeifer Johannes and Khanna, Raghav and Lottes, Philipp and Stachniss, Cyrill and Falck, Tillmann and Sander, Slawomir and Siegwart, Roland and Walter, Achim and Galceran, Enric},
  title         = {{Flourish -- A robotic approach for automation in crop management}},
  booktitle     = {In Proc.~of the Workshop f\"ur Computer-Bildanalyse und unbemannte autonom fliegende Systeme in der Landwirtschaft},
  year          = 2016,
}


@InProceedings{lottes2018iros,
  author =   {Philipp Lottes and Jens Behley and Nived Chebrolu and Andres Milioto and Cyrill Stachniss},
  title =    {{Joint Stem Detection and Crop-Weed Classification for Plant-specific Treatment in Precision Farming}},
  booktitle = iros,
  year =   2018,
  abstract = {Applying agrochemicals is the default procedure for conventional weed control in crop production, but has negative impacts on the environment. Robots have the potential to treat every plant in the field individually and thus can reduce the required use of such chemicals. To achieve that, robots need the ability to identify crops and weeds in the field and must additionally select effective treatments. While certain types of weed can be treated mechanically, other types need to be treated by (selective) spraying. In this paper, we present an approach that provides the necessary information for effective plant-specific treatment. It outputs the stem location for weeds, which allows for mechanical treatments, and the covered area of the weed for selective spraying. Our approach uses an end-to- end trainable fully convolutional network that simultaneously estimates stem positions as well as the covered area of crops and weeds. It jointly learns the class-wise stem detection and the pixel-wise semantic segmentation. Experimental evaluations on different real-world datasets show that our approach is able to reliably solve this problem. Compared to state-of-the-art approaches, our approach not only substantially improves the stem detection accuracy, i.e., distinguishing crop and weed stems, but also provides an improvement in the semantic segmentation performance.}
}

@Article{mccool2017ral,
  author        = {Chris McCool and Tristan Perez and Ben Upcroft},
  title         = {{Mixtures of Lightweight Deep Convolutional Neural Networks: Applied to Agricultural Robotics}},
  journal       = ral,
  year          = 2017,
  keywords      = {Computer Vision for Automation, Recognition, Agricultural Automation, CNN},
  abstract      = {We propose a novel approach for training deep convolutional neural networks (DCNNs) that allows us to tradeoff complexity and accuracy to learn lightweight models suitable for robotic platforms such as AgBot II (which performs automated weed management). Our approach consists of three stages, the first is to adapt a pre-trained model to the task at hand. This provides state-of-the-art performance but at the cost of high computational complexity resulting in a low frame rate of just 0.12 frames per second (fps). Second, we use the adapted model and employ model compression techniques to learn a lightweight DCNN that is less accurate but has two orders of magnitude fewer parameters. Third, K lightweight models are combined as a mixture model to further enhance the performance of the lightweight models. Applied to the challenging task of weed segmentation, we improve accuracy from 85.9\%, using a traditional approach, to 93.9\% by adapting a complicated pre-trained DCNN with 25M parameters (Inception-v3). The downside to this adapted model, Adapted-IV3, is that it can only process 0.12fps. To make this approach fast while still retaining accuracy, we learn lightweight DCNNs which when combined can achieve accuracy greater than 90\% while using considerably fewer parameters capable of processing between 1.07 and 1.83 frames per second, up to an order of magnitude faster and up to an order of magnitude fewer parameters.},
}

@InProceedings{milioto2017uavg,
Title = {Real-time Blob-wise Sugar Beets vs Weeds Classification for Monitoring Fields using Convolutional Neural Networks},
Author = {Andres Milioto and Philipp Lottes and Cyrill Stachniss},
Booktitle = uavg,
Year = {2017},
Abstract = {UAVs are becoming an important tool for field monitoring and precision farming. A prerequisite for observing and analyzing fields is the ability to identify crops and weeds from image data. In this paper, we address the problem of detecting the sugar beet plants and weeds in the field based solely on image data. We propose a system that combines vegetation detection and deep learning to obtain a high-quality classification of the vegetation in the field into value crops and weeds. We implemented and thoroughly evaluated our system on image data collected from different sugar beet fields and illustrate that our approach allows for accurately identifying the weeds on the field.},
url = {http://www.ipb.uni-bonn.de/pdfs/milioto17uavg.pdf}
}

@article{sa2018rs,
  author = {Inkyu Sa and Marija Popovic and Raghav Khanna and Zetao Chen and Philipp Lottes and Frank Liebisch and Juan Nieto and Cyrill Stachniss and Achim Walter and Roland Siegwart},
  title = {{WeedMap: A Large-Scale Semantic Weed Mapping Framework Using Aerial Multispectral Imaging and Deep Neural Network for Precision Farming}},
  journal = rs,
  year = 2018,
  volume =  10, 
  issue = 9,
  url = {http://www.mdpi.com/2072-4292/10/9/1423/pdf},
  doi = {10.3390/rs10091423},
  abstract =  {The ability to automatically monitor agricultural fields is an important capability in precision farming, enabling steps towards more sustainable agriculture. Precise, high-resolution monitoring is a key prerequisite for targeted intervention and the selective application of agro-chemicals. The main goal of this paper is developing a novel crop/weed segmentation and mapping framework that processes multispectral images obtained from an unmanned aerial vehicle (UAV) using a deep neural network (DNN).  Most studies on crop/weed semantic segmentation only consider single images for processing and classification. Images taken by UAVs often cover only a few hundred square meters with either color only or color and near-infrared (NIR) channels. Although a map can be generated by processing single segmented images incrementally, this requires additional complex information fusion techniques which struggle to handle high fidelity maps due to their computational costs and problems in ensuring global consistency. Moreover, computing a single large and accurate vegetation map (e.g., crop/weed) using a DNN is non-trivial due to difficulties arising from: (1) limited ground sample distances (GSDs) in high-altitude datasets, (2) sacrificed resolution resulting from downsampling high-fidelity images, and (3) multispectral image alignment. To address these issues, we adopt a stand sliding window approach that operates on only small portions of multispectral orthomosaic maps (tiles), which are channel-wise aligned and calibrated radiometrically across the entire map. We define the tile size to be the same as that of the DNN input to avoid resolution loss. Compared to our baseline model (i.e., SegNet with 3 channel RGB inputs) yielding an area under the curve (AUC) of [background=0.607, crop=0.681, weed=0.576], our proposed model with 9 input channels achieves [0.839, 0.863, 0.782]. Additionally, we provide an extensive analysis of 20 trained models, both qualitatively and quantitatively, in order to evaluate the effects of varying input channels and tunable network hyperparameters. Furthermore, we release a large sugar beet/weed aerial dataset with expertly guided annotations for further research in the fields of remote sensing, precision agriculture, and agricultural robotics.},
}

@inproceedings{DBLP:conf/iccv/LinGGHD17,
  author    = {Tsung{-}Yi Lin and
               Priya Goyal and
               Ross B. Girshick and
               Kaiming He and
               Piotr Doll{\'{a}}r},
  title     = {Focal Loss for Dense Object Detection},
  booktitle = iccv,
  pages     = {2999--3007},
  year      = {2017}
}

@article{du2018adapting,
  title={Adapting auxiliary losses using gradient similarity},
  author={Du, Yunshu and Czarnecki, Wojciech M and Jayakumar, Siddhant M and Pascanu, Razvan and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1812.02224},
  year={2018}
}


@inproceedings{wei2018revisiting,
  title={Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation},
  author={Wei, Yunchao and Xiao, Huaxin and Shi, Honghui and Jie, Zequn and Feng, Jiashi and Huang, Thomas S},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7268--7277},
  year={2018}
}


@inproceedings{acuna2018efficient,
  title={Efficient interactive annotation of segmentation datasets with polygon-rnn++},
  author={Acuna, David and Ling, Huan and Kar, Amlan and Fidler, Sanja},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={859--868},
  year={2018}
}

@inproceedings{tang2018normalized,
  title={Normalized cut loss for weakly-supervised CNN segmentation},
  author={Tang, Meng and Djelouah, Abdelaziz and Perazzi, Federico and Boykov, Yuri and Schroers, Christopher},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1818--1827},
  year={2018}
}

@inproceedings{kwak2017weakly,
  title={Weakly supervised semantic segmentation using superpixel pooling network},
  author={Kwak, Suha and Hong, Seunghoon and Han, Bohyung},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}




